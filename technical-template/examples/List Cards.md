
---
cssClass: minimal-float, wide-page
---

**reference**: https://efemkay.github.io/obsidian-modular-css-layout/showcases/mc-callout-list-card/
## Machine Learning Topics
- ### Support Vector Machines #mcl/list-card
	- ![SVM classification |right|300](https://gamedevacademy.org/wp-content/uploads/2017/10/SVM-RBF-Iris.png.webp)
	 **Support vector machines (SVMs)** are a set of supervised learning methods used for [classification](https://scikit-learn.org/stable/modules/svm.html#svm-classification), [regression](https://scikit-learn.org/stable/modules/svm.html#svm-regression) and [outliers detection](https://scikit-learn.org/stable/modules/svm.html#svm-outlier-detection).
	 The advantages of support vector machines are:
	  > -   Effective in high dimensional spaces.
	  > -   Still effective in cases where number of dimensions is greater than the number of samples.    
	  > -   Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.
	  > -   Versatile: different [Kernel functions](https://scikit-learn.org/stable/modules/svm.html#svm-kernels) can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.
- ### Convolutional Neural Networks
	- ![CNN Encoder-Decoder|right|300](https://i0.wp.com/neptune.ai/wp-content/uploads/2022/10/Image-segmentation-architectures.png?ssl=1)
	 In [deep learning](https://en.wikipedia.org/wiki/Deep_learning "Deep learning"), a **convolutional neural network** (**CNN**, or **ConvNet**) is a class of [artificial neural network](https://en.wikipedia.org/wiki/Artificial_neural_network "Artificial neural network") (**ANN**), most commonly applied to analyze visual imagery.<sup id="cite_ref-Valueva_Nagornov_Lyakhov_Valuev_2020_pp._232–243_1-0"><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#cite_note-Valueva_Nagornov_Lyakhov_Valuev_2020_pp._232%E2%80%93243-1">[1]</a></sup> CNNs are also known as **Shift Invariant** or **Space Invariant Artificial Neural Networks** (**SIANN**), based on the shared-weight architecture of the [convolution](https://en.wikipedia.org/wiki/Convolution "Convolution") kernels or filters that slide along input features and provide translation-[equivariant](https://en.wikipedia.org/wiki/Equivariant_map "Equivariant map") responses known as feature maps.<sup id="cite_ref-:0_2-0"><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#cite_note-:0-2">[2]</a></sup><sup id="cite_ref-:1_3-0"><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#cite_note-:1-3">[3]</a></sup> Counter-intuitively, most convolutional neural networks are not [invariant](https://en.wikipedia.org/wiki/Translation_invariant "Translation invariant") to translation, due to the downsampling operation they apply to the input.<sup id="cite_ref-:6_4-0"><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#cite_note-:6-4">[4]</a></sup> They have applications in [image and video recognition](https://en.wikipedia.org/wiki/Computer_vision "Computer vision"), [recommender systems](https://en.wikipedia.org/wiki/Recommender_system "Recommender system"),<sup id="cite_ref-5"><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#cite_note-5">[5]</a></sup> [image classification](https://en.wikipedia.org/wiki/Image_classification "Image classification"), [image segmentation](https://en.wikipedia.org/wiki/Image_segmentation "Image segmentation"), [medical image analysis](https://en.wikipedia.org/wiki/Medical_image_computing "Medical image computing"), [natural language processing](https://en.wikipedia.org/wiki/Natural_language_processing "Natural language processing"),<sup id="cite_ref-6"><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#cite_note-6">[6]</a></sup> [brain–computer interfaces](https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface "Brain–computer interface"),<sup id="cite_ref-7"><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#cite_note-7">[7]</a></sup> and financial [time series](https://en.wikipedia.org/wiki/Time_series "Time series").<sup id="cite_ref-Tsantekidis_7–12_8-0"><a href="https://en.wikipedia.org/wiki/Convolutional_neural_network#cite_note-Tsantekidis_7%E2%80%9312-8">[8]</a></sup>
- ### Recurrent Neural Networks
	- ![RNN architecture|right|300](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs42256-021-00297-z/MediaObjects/42256_2021_297_Fig1_HTML.png)
	 **Long short-term memory** (**LSTM**)<sup id="cite_ref-lstm1997_1-0"><a href="https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-lstm1997-1">[1]</a></sup> is an [artificial neural network](https://en.wikipedia.org/wiki/Artificial_neural_network "Artificial neural network") used in the fields of [artificial intelligence](https://en.wikipedia.org/wiki/Artificial_intelligence "Artificial intelligence") and [deep learning](https://en.wikipedia.org/wiki/Deep_learning "Deep learning"). Unlike standard [feedforward neural networks](https://en.wikipedia.org/wiki/Feedforward_neural_network "Feedforward neural network"), LSTM has feedback connections. Such a [recurrent neural network](https://en.wikipedia.org/wiki/Recurrent_neural_network "Recurrent neural network") (RNN) can process not only single data points (such as images), but also entire sequences of data (such as speech or video). For example, LSTM is applicable to tasks such as unsegmented, connected [handwriting recognition](https://en.wikipedia.org/wiki/Handwriting_recognition "Handwriting recognition"),<sup id="cite_ref-graves2009_2-0"><a href="https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-graves2009-2">[2]</a></sup> [speech recognition](https://en.wikipedia.org/wiki/Speech_recognition "Speech recognition"),<sup id="cite_ref-sak2014_3-0"><a href="https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-sak2014-3">[3]</a></sup><sup id="cite_ref-liwu2015_4-0"><a href="https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-liwu2015-4">[4]</a></sup> [machine translation](https://en.wikipedia.org/wiki/Machine_translation "Machine translation"),<sup id="cite_ref-GoogleTranslate_5-0"><a href="https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-GoogleTranslate-5">[5]</a></sup><sup id="cite_ref-FacebookTranslate_6-0"><a href="https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-FacebookTranslate-6">[6]</a></sup> robot control,<sup id="cite_ref-mayer2006_7-0"><a href="https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-mayer2006-7">[7]</a></sup><sup id="cite_ref-OpenAIhand_8-0"><a href="https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-OpenAIhand-8">[8]</a></sup> video games,<sup id="cite_ref-OpenAIfive_9-0"><a href="https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-OpenAIfive-9">[9]</a></sup><sup id="cite_ref-alphastar_10-0"><a href="https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-alphastar-10">[10]</a></sup> and healthcare.<sup id="cite_ref-decade2022_11-0"><a href="https://en.wikipedia.org/wiki/Long_short-term_memory#cite_note-decade2022-11">[11]</a></sup>
- ### Generative Adversarial Networks
	- ![GAN|right|300](https://lilianweng.github.io/posts/2017-08-20-gan/GAN.png)
	 A **generative adversarial network** (**GAN**) is a class of [machine learning](https://en.wikipedia.org/wiki/Machine_learning "Machine learning") frameworks designed by [Ian Goodfellow](https://en.wikipedia.org/wiki/Ian_Goodfellow "Ian Goodfellow") and his colleagues in June 2014.<sup id="cite_ref-GANnips_1-0"><a href="https://en.wikipedia.org/wiki/Generative_adversarial_network#cite_note-GANnips-1">[1]</a></sup> Two [neural networks](https://en.wikipedia.org/wiki/Neural_network "Neural network") contest with each other in the form of a [zero-sum game](https://en.wikipedia.org/wiki/Zero-sum_game "Zero-sum game"), where one agent's gain is another agent's loss.

	 Given a training set, this technique learns to generate new data with the same statistics as the training set. For example, a GAN trained on photographs can generate new photographs that look at least superficially authentic to human observers, having many realistic characteristics. Though originally proposed as a form of [generative model](https://en.wikipedia.org/wiki/Generative_model "Generative model") for [unsupervised learning](https://en.wikipedia.org/wiki/Unsupervised_learning "Unsupervised learning"), GANs have also proved useful for [semi-supervised learning](https://en.wikipedia.org/wiki/Semi-supervised_learning "Semi-supervised learning"),<sup id="cite_ref-ITT_GANs_2-0"><a href="https://en.wikipedia.org/wiki/Generative_adversarial_network#cite_note-ITT_GANs-2">[2]</a></sup> fully [supervised learning](https://en.wikipedia.org/wiki/Supervised_learning "Supervised learning"),<sup id="cite_ref-3"><a href="https://en.wikipedia.org/wiki/Generative_adversarial_network#cite_note-3">[3]</a></sup> and [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning "Reinforcement learning").<sup id="cite_ref-4"><a href="https://en.wikipedia.org/wiki/Generative_adversarial_network#cite_note-4">[4]</a></sup>
	
	 The core idea of a GAN is based on the "indirect" training through the discriminator, another neural network that can tell how "realistic" the input seems, which itself is also being updated dynamically.<sup id="cite_ref-5"><a href="https://en.wikipedia.org/wiki/Generative_adversarial_network#cite_note-5">[5]</a></sup> This means that the generator is not trained to minimize the distance to a specific image, but rather to fool the discriminator. This enables the model to learn in an unsupervised manner.
